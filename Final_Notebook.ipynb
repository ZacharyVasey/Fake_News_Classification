{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_Notebook.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FtWdJd8kJIU6","colab_type":"text"},"source":["Team 14: Zachary Almaraz, Caleb Fowler, Jonathan Pierre, Zachary Vasey"]},{"cell_type":"markdown","metadata":{"id":"t8pUZamrye-6","colab_type":"text"},"source":["Notes\n","\n","Create a link to shared MachineLearningProject in personal drive."]},{"cell_type":"code","metadata":{"id":"oNHUZfi5puOb","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My\\ Drive/MachineLearningProject/\n","%ls -la"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48VaAIq8yugw","colab_type":"text"},"source":["Inspect data directory."]},{"cell_type":"code","metadata":{"id":"maMpVyQQyxfJ","colab_type":"code","colab":{}},"source":["%ls data -la"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ltTaUKK4xGKn","colab_type":"text"},"source":["Import news.csv dataset."]},{"cell_type":"code","metadata":{"id":"uqV1sb-Pwg1l","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df_original = pd.read_csv('data/news.csv') \n","datasets = [ {'name' : 'news.csv',     'df' : df_original}, ]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtWXI2ABeB0N","colab_type":"text"},"source":["Look at data."]},{"cell_type":"code","metadata":{"id":"pEfqNZyOeFQO","colab_type":"code","colab":{}},"source":["from IPython.display import display\n","\n","def inspect(datasets):\n","  for d in datasets:\n","    print('-' * 25 + '\\n' + d[\"name\"] + '\\n' + '-' * 25)\n","    print('Shape: ', end='')\n","    print(d[\"df\"].shape)\n","    print()\n","    print('Missing values:')\n","    print(d[\"df\"].isna().sum())\n","    print()\n","    print(\"first 3 samples:\")\n","    display(d[\"df\"].head(n=3))\n","\n","inspect(datasets)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCXCGWKJMLI_","colab_type":"text"},"source":["Drop samples with missing features."]},{"cell_type":"code","metadata":{"id":"hZXAqG02MG-k","colab_type":"code","colab":{}},"source":["df_original = df_original.dropna()\n","datasets = [ {'name' : 'news.csv',     'df' : df_original}, ]\n","inspect( datasets )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DTrqkStadr54","colab_type":"text"},"source":["Remove Duplicates by Title"]},{"cell_type":"code","metadata":{"id":"CXSPPobdP9zj","colab_type":"code","colab":{}},"source":["len_before = len(df_original['title'])\n","df_original.drop_duplicates(subset =\"title\", keep = 'first', inplace = True) \n","len_after = len(df_original['title'])\n","print(f'number of duplicate articles removed: {len_before - len_after}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1jbOu2DOp6l","colab_type":"text"},"source":["Feature Extraction"]},{"cell_type":"code","metadata":{"id":"gjeulxEEOus_","colab_type":"code","colab":{}},"source":["df = df_original.copy()\n","\n","new_dimensions = ['fake']\n","\n","for feature in ('title', 'text'):\n","  num_chars = []\n","  num_words = []\n","  size_words = []\n","  num_words_uppercase = []  # number of words consisting of all capital letters\n","\n","  for i in df[feature]:\n","    temp = []\n","    num_uppercase = 0\n","    test = 0\n","    for word in i.split(' '):\n","      temp.append(len(word))\n","      if word.isupper():\n","        if len(word) > 1:  # not counting \"I\" or \"A\" etc\n","          num_uppercase = num_uppercase + 1\n","\n","    size_words.append(sum(temp) / len(temp))\n","    num_words.append(len(i.split(' ')))\n","    num_chars.append(len(i))\n","    num_words_uppercase.append(num_uppercase)\n","\n","  names = ['avg_word_len_'+feature,\n","           'num_words_'+feature,\n","           'total_chars_'+feature,\n","           'words_uppercase_'+feature]\n","\n","  new_dimensions.extend(names)\n","\n","  df[names[0]] = size_words\n","  df[names[1]] = num_words\n","  df[names[2]] = num_chars\n","  df[names[3]] = num_words_uppercase\n","\n","df = df[new_dimensions]\n","\n","print('done')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxzqDKAwXjur","colab_type":"text"},"source":["Inspect the new dataframe."]},{"cell_type":"code","metadata":{"id":"NDHZ-X_GXues","colab_type":"code","colab":{}},"source":["inspect(datasets = [{'name' : 'news_modified', 'df' : df},])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_WWwJoo3e1sn","colab_type":"text"},"source":["Examine the \"uppercase words\" dimension."]},{"cell_type":"code","metadata":{"id":"SZMJ2Cxb47cM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.mlab as mlab\n","import matplotlib.pyplot as plt\n","\n","real = df[df.fake == 0]\n","fake = df[df.fake == 1]\n","\n","# title feature\n","a = real.words_uppercase_title\n","b = fake.words_uppercase_title\n","\n","f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), sharey=True)\n","f.suptitle(\"title: uppercase words\", fontsize=20)\n","n, bins, patches = ax1.hist(a, max(a), facecolor='blue', alpha=0.7)\n","ax1.set_title('Real News')\n","ax1.set_ylabel(\"samples\")\n","ax1.set_xlabel('uppercase words')\n","\n","ax2.set_title('Fake News')\n","n, bins, patches = ax2.hist(b, max(b), facecolor='red', alpha=0.7)\n","ax2.set_xlabel('uppercase words')\n","\n","plt.show()\n","\n","# text feature\n","c = real.words_uppercase_text\n","d = fake.words_uppercase_text\n","\n","f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5), sharey=True)\n","f.suptitle(\"text: uppercase words\", fontsize=20)\n","n, bins, patches = ax1.hist(c, max(c), facecolor='blue', alpha=0.7)\n","ax1.set_title('Real News')\n","ax1.set_ylabel(\"samples\")\n","ax1.set_xlabel('uppercase words')\n","\n","ax2.set_title('Fake News')\n","n, bins, patches = ax2.hist(d, max(d), facecolor='red', alpha=0.7)\n","ax2.set_xlabel('uppercase words')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcUGINyae2Fw","colab_type":"code","colab":{}},"source":["real = df[df.fake == 0]\n","fake = df[df.fake == 1]\n","\n","count_real_news = len(real)\n","count_fake_news = len(fake)\n","\n","print(f\"total real news articles: {count_real_news}\")\n","print(f\"total fake news articles: {count_fake_news}\")\n","\n","real_uppercase_title = []\n","real_uppercase_text = []\n","fake_uppercase_title = []\n","fake_uppercase_text = []\n","\n","nmax = 7\n","for n in range(0, nmax):\n","  # real articles that have at least n all uppercase words\n","  real_uppercase_title.append( real[real.words_uppercase_title > n] )\n","  real_uppercase_text.append( real[real.words_uppercase_text > n] ) \n","\n","  # fake articles that have at least n all uppercase words\n","  fake_uppercase_title.append( fake[fake.words_uppercase_title > n] )\n","  fake_uppercase_text.append( fake[fake.words_uppercase_text > n] )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RY4CvD4wpD0e","colab_type":"code","colab":{}},"source":["# Visualize results\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from matplotlib.ticker import MaxNLocator\n","\n","x = np.arange(0,nmax,1)\n","\n","a = [len(i) for i in real_uppercase_title]\n","b = [len(i) for i in fake_uppercase_title]\n","c = [len(i) for i in real_uppercase_text]\n","d = [len(i) for i in fake_uppercase_text]\n","\n","fig = plt.figure(figsize=(14,5))\n","fig.suptitle('title: uppercase words')\n","\n","ax = fig.add_subplot(111)  # 121\n","ax.plot(x, a, 'bs')\n","ax.plot(x, b, 'rs')\n","plt.ylabel(\"total samples\")\n","plt.xlabel(\"total uppercase words > x\")\n","ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n","\n","ax.legend(('real news', 'fake news'))\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xmkm6gL0XdJS","colab_type":"text"},"source":["Normalize data function."]},{"cell_type":"code","metadata":{"id":"-TMTpPCnXcyq","colab_type":"code","colab":{}},"source":["def normalize_data(df, realcols):\n","  for col in realcols:\n","    df[col] = (df[col] - df[col].mean()) / df[col].std()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9J_bRyi7KVrq","colab_type":"text"},"source":["Normalize data."]},{"cell_type":"code","metadata":{"id":"YsWtZivGKV_R","colab_type":"code","colab":{}},"source":["dimensions_to_normalize = new_dimensions.copy()\n","\n","try:\n","  dimensions_to_normalize.remove('fake')\n","except ValueError as e:\n","  pass\n","\n","dfn = df.copy()\n","normalize_data(dfn, dimensions_to_normalize)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iGVz5VBhLbwT","colab_type":"text"},"source":["Display normalized data."]},{"cell_type":"code","metadata":{"id":"3GuM20_WLci7","colab_type":"code","colab":{}},"source":["dfn.sample(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Q6-NGCPNmzs","colab_type":"text"},"source":["Get X and y."]},{"cell_type":"code","metadata":{"id":"5SkEO9zuM7Or","colab_type":"code","colab":{}},"source":["y = dfn['fake']\n","df_temp = dfn.copy()\n","df_temp.drop('fake', axis=1, inplace=True)\n","X = df_temp"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rrjExCnPWoH","colab_type":"text"},"source":["Visualize features on subset of samples"]},{"cell_type":"code","metadata":{"id":"mGYfMYtQPePf","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","\n","np.random.seed(4347)\n","small_df = df.sample(int(0.0025*len(df)))\n","\n","print('label size', f'true {(small_df.fake==0).sum()}, fake {(small_df.fake==1).sum()}')\n","sns.set(style=\"ticks\", color_codes=True)\n","sns.pairplot(small_df, hue = 'fake', kind = 'scatter', markers=[\"o\", \"s\"], corner=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhtWNuSTOzFz","colab_type":"text"},"source":["Train Test Split"]},{"cell_type":"markdown","metadata":{"id":"qvtV5KJBP6cf","colab_type":"text"},"source":["Initial Comparison of various classifiers using a K-FOLD = 25"]},{"cell_type":"code","metadata":{"id":"J4z1QERMAmJd","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from sklearn import model_selection\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import  DecisionTreeClassifier, plot_tree\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn import linear_model\n","\n","# prepare configuration for cross validation test harness\n","seed = 7\n","\n","# prepare models\n","models = []\n","models.append(('LR', LogisticRegression()))\n","models.append(('LDA', LinearDiscriminantAnalysis()))\n","models.append(('QDA', QuadraticDiscriminantAnalysis()))\n","models.append(('KNN', KNeighborsClassifier(n_neighbors=41)))\n","models.append(('Tree', DecisionTreeClassifier()))\n","models.append(('GNB', GaussianNB()))\n","\n","# evaluate each model in turn\n","results = []\n","names = []\n","scoring = 'accuracy'\n","for name, model in models:\n","\tkfold = model_selection.KFold(n_splits=25)\n","\tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n","\tresults.append(cv_results)\n","\tnames.append(name)\n","\tmsg = \"%s: mean: %f, std: %f\" % (name, cv_results.mean(), cv_results.std())\n","\tprint(msg)\n"," \n","# boxplot algorithm comparison\n","fig = plt.figure()\n","fig.suptitle('Algorithm Comparison')\n","ax = fig.add_subplot(111)\n","plt.boxplot(results)\n","ax.set_xticklabels(names)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kEUPpN_v8BJf","colab_type":"text"},"source":["KNN Classifier"]},{"cell_type":"code","metadata":{"id":"QqW2cfFW7e6U","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","y = df['fake']\n","df_temp = df.copy()\n","df_temp.drop('fake', axis=1, inplace=True)\n","X = df_temp\n","\n","def returnScore(k, xtrain, xtest, ytrain, ytest):\n","  knc = KNeighborsClassifier(n_neighbors=k)\n","  knc.fit(xtrain, ytrain)\n","  return knc.score(xtest, ytest)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, stratify=y, random_state=42)\n","\n","result = []\n","test_k = range(1, 51, 1)\n","for i in test_k:\n","  result.append(returnScore(i, X_train, X_test, y_train, y_test))\n","plt.plot(test_k, result)\n","plt.title(\"KNN Score with different K values. test_size=0.05\")\n","plt.ylabel(\"Score\")\n","plt.xlabel(\"K\")\n","plt.show()\n","best_k = result.index(max(result)) + 1\n","print(f'highest score: {max(result)} with {best_k} k')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_FRubuytPoYR","colab_type":"text"},"source":["Testing KNN with different Test Sizes"]},{"cell_type":"code","metadata":{"id":"PpbURVlIPmpX","colab_type":"code","colab":{}},"source":["#test size 0.01 - 0.1\n","\n","best_k_list = []\n","test_sizes = []\n","for i in range(1, 11):\n","  test_sizes.append(float(i/100))\n","print(test_sizes)\n","for test_size in test_sizes:\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n","\n","  result = []\n","  test_k = range(1, 101, 2)\n","  for i in test_k:\n","    #print(i)\n","    result.append(returnScore(i, X_train, X_test, y_train, y_test))\n","  plt.plot(test_k, result)\n","  plt.title(f'KNN Score with different K values. test_size={test_size}')\n","  plt.ylabel(\"Score\")\n","  plt.xlabel(\"K\")\n","  plt.show()\n","  best_k = (result.index(max(result)) * 2) + 1\n","  print(f'highest score: {max(result)} with {best_k} k')\n","  best_k_list.append(best_k)\n","\n","plt.scatter(test_sizes, best_k_list)\n","plt.title(f'Best K with each test_size')\n","plt.ylabel(\"Best K\")\n","plt.xlabel(\"Test Size\")\n","plt.show()\n","print(best_k_list.sort())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t51RpB7VNDFe","colab_type":"text"},"source":["Cross Validation to find optimal k"]},{"cell_type":"code","metadata":{"id":"2QrWaYQNHQFI","colab_type":"code","colab":{}},"source":["cv_values = [5, 10]\n","k_range = []\n","total_scores = []\n","for i in range(1, 101):\n","  k_range.append(i)\n","\n","for i in k_range:\n","  knc = KNeighborsClassifier(n_neighbors=i)\n","  scores_mean = []\n","  scores_max = []\n","  scores_min = []\n","  \n","  for i in cv_values: \n","    scores = cross_val_score(knc, X, y, cv=i)\n","    scores_mean.append(scores.mean())\n","    scores_max.append(max(scores))\n","    scores_min.append(min(scores))\n","  \n","  total_scores.append(sum(scores_mean) / len(scores_mean))\n","\n","plt.figure()\n","plt.plot(k_range, total_scores, color=\"blue\", linewidth=2)\n","plt.xlabel(\"k\")\n","plt.ylabel(\"score\")\n","plt.title(\"k vs scores\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m5Lo5LqvXgb8","colab_type":"text"},"source":["Display best score"]},{"cell_type":"code","metadata":{"id":"O4xhS0yjXizc","colab_type":"code","colab":{}},"source":["print(f'Best Score: {max(total_scores)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBxwOLh4xX89","colab_type":"text"},"source":["Testing KNN with CV"]},{"cell_type":"code","metadata":{"id":"n20NbQVgxh-m","colab_type":"code","colab":{}},"source":["for i in range(1, 10):\n","  print(f'\\nK = {i}\\n')\n","  knc = KNeighborsClassifier(n_neighbors=i)\n","  scores_mean = []\n","  scores_max = []\n","  scores_min = []\n","  cv_range = range(2, 25, 1)\n","\n","  for i in cv_range: \n","    scores = cross_val_score(knc, X, y, cv=i)\n","    scores_mean.append(scores.mean())\n","    scores_max.append(max(scores))\n","    scores_min.append(min(scores))\n","    print(f'cv = {i}, score mean = {scores.mean()}, max = {max(scores)}, min = {min(scores)}')\n","\n","  plt.figure()\n","  plt.plot(cv_range, scores_mean, color=\"purple\", linewidth=2)\n","  plt.plot(cv_range, scores_min, color=\"red\", linewidth=2)\n","  plt.plot(cv_range, scores_max, color=\"blue\", linewidth=2)\n","  plt.xlabel(\"k\")\n","  plt.ylabel(\"score\")\n","  plt.title(\"k vs scores\")\n","  plt.legend()\n","  plt.show()\n","\n","  best_cv = scores_mean.index(max(scores_mean)) + 2\n","  print(f'highest score: {max(scores_mean)} with {best_cv} k')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6S7yLFRzML7P","colab_type":"text"},"source":["KNN Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"f1D8HazNXAkZ","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n","\n","labels = ['True', 'False']\n","\n","def convert(i):\n","  return labels[i]\n","knc = KNeighborsClassifier(n_neighbors=best_k)\n","knc.fit(X_train, y_train)\n","ytest_pred = knc.predict(X_test)\n","\n","ytest_pred = list(map(convert, ytest_pred))\n","ytest_true = list(map(convert, y_test))\n","print(ytest_pred)\n","print(ytest_true)\n","cmatrix = confusion_matrix(ytest_true, ytest_pred, labels=labels) #use labels name as well\n","\n","print(cmatrix)\n","\n","sns.heatmap(data = cmatrix, xticklabels = labels, yticklabels = labels, annot = True, cbar = True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2KkFhD60mVj","colab_type":"text"},"source":["Cross Validation Score Function for use by SVM"]},{"cell_type":"code","metadata":{"id":"Ti8aOfo1N-4x","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn import model_selection\n","\n","def models_cross_val(models, k_list, X, y):\n","  results = []\n","  names = []\n","  for name, model in models:\n","    print()\n","    print(name)\n","    for k in (k_list):\n","      names.append(name + ' k=' + str(k))\n","      kfold = model_selection.KFold(n_splits=k)\n","      scores = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n","      results.append(scores)\n","      print(f'k = {k}: mean = {scores.mean()}, std = {scores.std()}, max = {max(scores)}, min = {min(scores)}')\n","  return results, names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8b9H0ilzPoN2","colab_type":"text"},"source":["SVM kernels"]},{"cell_type":"code","metadata":{"id":"125GF3HzOFVn","colab_type":"code","colab":{}},"source":["print('Cross Validation Scores for SVM kernels')\n","\n","models = []\n","models.append(('linear' , SVC(kernel='linear')))\n","models.append(('poly' , SVC(kernel='poly')))\n","models.append(('rbf' , SVC(kernel='rbf')))\n","models.append(('sigmoid' , SVC(kernel='sigmoid')))\n","\n","# k_list = [5,10]\n","k_list = [10]\n","\n","results_svm, names_svm = models_cross_val(models, k_list, X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5iM5UAHfwf_-","colab_type":"text"},"source":["Plot."]},{"cell_type":"code","metadata":{"id":"YDp3oB6MxQio","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","labels = [5, 10]\n","\n","fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5), sharey=True, )\n","\n","# linear\n","bplot1 = axes[0].boxplot( [results_svm[0], results_svm[1]] ,\n","                         vert=True,  # vertical box alignment\n","                         patch_artist=True,  # fill with color\n","                         labels=labels)  # will be used to label x-ticks\n","axes[0].set_title('linear')\n","\n","# poly\n","bplot2 = axes[1].boxplot( [results_svm[2], results_svm[3]] ,\n","                         vert=True,\n","                         patch_artist=True,\n","                         labels=labels)\n","axes[1].set_title('poly')\n","\n","# rbf\n","bplot2 = axes[2].boxplot( [results_svm[4], results_svm[5]] ,\n","                         vert=True,\n","                         patch_artist=True,\n","                         labels=labels)\n","axes[2].set_title('rbf')\n","\n","# sigmoid\n","bplot2 = axes[3].boxplot( [results_svm[6], results_svm[7]] ,\n","                         vert=True,\n","                         patch_artist=True,\n","                         labels=labels)\n","axes[3].set_title('sigmoid')\n","\n","\n","# fill with colors\n","colors = ['lightblue', 'lightgreen']\n","for bplot in (bplot1, bplot2):\n","    for patch, color in zip(bplot['boxes'], colors):\n","        patch.set_facecolor(color)\n","\n","# horizontal grid lines\n","for ax in axes:\n","    ax.yaxis.grid(True)\n","    ax.set_xlabel('k-fold')\n","    ax.set_ylabel('Score')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nsh3Cg5OUY8e","colab_type":"text"},"source":["SVC parameter selection function\n"]},{"cell_type":"code","metadata":{"id":"_1ItRhW6UZPI","colab_type":"code","colab":{}},"source":["from sklearn import svm\n","from sklearn.model_selection import GridSearchCV\n","\n","def svc_param_selection(X, y, nfolds):\n","    Cs = [0.1, 1, 10]\n","    gammas = [0.1, 1, 10]\n","    param_grid = {'C': Cs, 'gamma' : gammas}\n","    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n","    grid_search.fit(X, y)\n","    grid_search.best_params_\n","    return grid_search.best_params_, grid_search.best_score_, grid_search\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wm7m-nWAWOhi","colab_type":"text"},"source":["Display the best SVC Tuning parameters"]},{"cell_type":"code","metadata":{"id":"MvlYWzg0WTOi","colab_type":"code","colab":{}},"source":["params, score, grid_search = svc_param_selection(X, y, 5)\n","print(params)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l86Ry4IFx2N9","colab_type":"code","colab":{}},"source":["print(f'{score}')\n","\n","models = []\n","models.append(('rbf' , SVC(kernel='rbf', C=1, gamma=1)))\n","k_list = [5,10,25]\n","\n","results_svm, names_svm = models_cross_val(models, k_list, X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I6xbcGdl1QkL","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","labels = [5, 10, 25]\n","\n","fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5) )\n","\n","# rbf\n","bplot1 = axes.boxplot( [ results_svm[0], results_svm[1], results_svm[2] ],\n","                         vert=True,  # vertical box alignment\n","                         patch_artist=True,  # fill with color\n","                         labels=labels)  # will be used to label x-ticks\n","axes.set_title('rbf kernel cv scores (c=1, gamma=1)')\n","\n","# fill with colors\n","colors = ['lightblue', 'lightgreen', 'lightpink']\n","for patch, color in zip(bplot1['boxes'], colors):\n","    patch.set_facecolor(color)\n","\n","# horizontal grid lines\n","axes.yaxis.grid(True)\n","axes.set_xlabel('k-fold')\n","axes.set_ylabel('Score')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"chD2H7MCRrxb","colab_type":"code","colab":{}},"source":["Train Test Split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2p6FFb3EO2dl","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UgA0looeQNMT","colab_type":"text"},"source":["SVM kernel - determine Overfitting"]},{"cell_type":"code","metadata":{"id":"CrW-8cobQVf7","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","\n","svc = SVC(kernel='rbf', random_state=42)\n","svc.fit(X_train, y_train)\n","\n","print(f'training score: {svc.score(X_train, y_train)}')\n","print(f'testing score: {svc.score(X_test, y_test)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITjgzuv7QwvL","colab_type":"text"},"source":["ROC Curve for SVM Train/Test split"]},{"cell_type":"code","metadata":{"id":"2AEoxOBBQ6st","colab_type":"code","colab":{}},"source":["from sklearn.metrics import plot_roc_curve\n","plot_roc_curve(svc, X_test, y_test)\n","\n","import matplotlib.pyplot as plt\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EYLLmzw4bBrg","colab_type":"text"},"source":["Naive Bayes\n"]},{"cell_type":"markdown","metadata":{"id":"pFqZTpd3cN_p","colab_type":"text"},"source":["Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"9nZ-1oDQaWyl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import string\n","import re\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","nltk_stopwords = stopwords.words('english')\n","remove_punctuation = '!\"$%&\\'()*+,-./:;<=>?@[\\\\]‚Äú‚Äù^_`{|}~‚Äô'\n","\n","def clean_column(dataframe, column_to_clean, new_col):\n","    df_copy = dataframe.copy()\n","    df_copy['copied_column'] = df_copy[column_to_clean]\n","    df_copy['copied_column'] = df_copy['copied_column'].str.lower()\n","    cleaned_column = []\n","    for label in df_copy.index:\n","        row = df_copy.loc[label, :]['copied_column']\n","        clean = [x for x in row.split() if x not in string.punctuation]\n","        clean = [x for x in clean if x not in nltk_stopwords]\n","        clean = [x for x in clean if x not in string.digits]\n","        clean = [x for x in clean if x not in remove_punctuation]\n","        clean = [x for x in clean if len(x) != 1]\n","        clean = \" \".join(clean)\n","        clean = clean.strip()\n","        cleaned_column.append(clean)\n","    df_copy[new_col] = cleaned_column\n","    del df_copy['copied_column']\n","    return df_copy\n","\n","def filtration(dataframe, column):\n","    # clean = list(map(lambda x: x.replace(\"#\", \"\"), clean)) #we want to maintain hashtags!\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace('\"', \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚Äô\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\":\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚Ä¶\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\".\",\"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚ãÜ\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ‚ãÜ \", \" \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"  \", \" \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"$\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\",\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" alime \", \" all time \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" alltime \", \" all time \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\";\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"alime\", \"all time \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"atm\", \"at the moment\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ath \", \" all time high \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"str8\", \"straight\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" v \", \" very \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" #d\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ddos \", \" distributed denial of service \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"btce\", \"btc\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"bitcoina\", \"bitcoin\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"rbitcoin\", \"bitcoin\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ‚Äì \", \" \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"-&gt;\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ‚û§ \", \" \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚óÑ‚ñ∫\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚óÑ\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ur \", \" your \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" u \", \" you \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"forthen\", \"for then\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"&gt;\", \"greater than\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"&lt;\", \"less than\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"lt\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"gt\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\":\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"&amp;\", \"and\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"ampamp\", \"and\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" amp \", \" and \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"amp\", \"and\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" bu \", \" but \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"/\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"...\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"(\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\")\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚Äú\", '\"'))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚Äù\", '\"'))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚Äò\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚Äô\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"-\",\" \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"*\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"!\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚¨õÔ∏è\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\u200d\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\U0001f986\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\U0001f942\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\U0001f92f\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\U0001f911\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\U0001F193\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ‚≠ï \", \" \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"ü§î\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"‚òû \", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"[\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"]\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"{\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"}\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√¥\", \"o\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√≥\", \"o\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√©\", \"e\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√Ø\",\"i\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"¬Æ\", \"\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√°\", \"a\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√£\", \"a\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"√ß\", \"c\"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" jan \", \" january \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" feb \", \" february \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" mar \", \" march \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" apr \", \" april \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" jun \", \" june \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" jul \", \" july \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" aug \", \" august \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" sept \", \" september \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" oct \", \" october \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" nov \", \" november \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" dec \", \" december \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" washinon \", \" washington \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" dming \", \" direct messaging \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" cust \", \" customer \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" wcust \", \" with customer \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" cc \", \" credit card \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" gopros \", \" go pros \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ultimatelyi \", \" ultimately i \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 1hr \", \" one hour \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" rep \", \" representative \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" wunited \", \" with united \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" mp# \", \" mileage plus number \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" hrs \", \" hours \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 4hours \", \" four hours \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" laxewr \", \" lax ewr \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" iadlax \", \" iad lax \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" julystill \", \" july still \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 30mins \", \" 30 minutes \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" mins \", \" minutes \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 5hours \", \" 5 hours \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" checkhowever \", \" check however \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" familyno \", \" family \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 2nd \", \" second \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 6hour \", \" six hour \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" cuz \", \" because \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" cause \", \" because \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" ideabuy \", \" idea buy \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" fixem \", \" fix them \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" properthey \", \" proper they \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" americanair \", \" american air \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" yea \", \" yes \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" gnteed \", \" guaranteed \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 6mo \", \" 6 months \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" believei \", \" believe \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" btw \", \" by the way \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" intl \", \" international \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" thxs \", \" thanks \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" plususual \", \" plus usual \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" fridaycant \", \" friday can not \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" lhr \", \" 1 hour \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" wheelsup \", \" wheels up \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" tryna \", \" try and \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 2hours \", \" 2 hours \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" 1st \", \" first \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" creditcard \", \" credit card \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" luv \", \" love \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" obv \", \" obviously \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" patientyou \", \" patient you \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" youwe \", \" you have \"))\n","    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\" uraniumone \", \" uranium one \"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"azN44_ieaXQG","colab_type":"code","colab":{}},"source":["df1 = df_original \n","df1.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKF0o62yaXhw","colab_type":"code","colab":{}},"source":["df1 = clean_column(df1, 'title', 'clean_title')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jaQ7hd-aYI8","colab_type":"code","colab":{}},"source":["df1 = clean_column(df1, 'text', 'clean_text')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gj9mcA_VaYYT","colab_type":"code","colab":{}},"source":["filtration(df1, 'clean_title')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2r8LBTqXaYoT","colab_type":"code","colab":{}},"source":["filtration(df1, 'clean_text')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eb8bhi2aY7B","colab_type":"code","colab":{}},"source":["df1.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzchGwonaZKT","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from collections import Counter\n","import nltk\n","from wordcloud import WordCloud\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWCgUh0AaZZb","colab_type":"code","colab":{}},"source":["X_body_text = df1['clean_text'].values\n","X_title_text = df1['clean_title'].values\n","y = df1['fake'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9owDpD9uaZoF","colab_type":"code","colab":{}},"source":["tfidf = TfidfVectorizer(ngram_range=(1,2), max_df= 0.85, min_df= 0.01)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"quXSyupzaZ4f","colab_type":"code","colab":{}},"source":["X_body_tfidf = tfidf.fit_transform(X_body_text)\n","X_title_tfidf = tfidf.fit_transform (X_title_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiEmj0TnaaFv","colab_type":"code","colab":{}},"source":["indices = df1.index.values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIcVXw8DaaUB","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"RreGuPXUaauj","colab_type":"code","colab":{}},"source":["X_body_tfidf_train, X_body_tfidf_test, \\\n","y_body_train, y_body_test, \\\n","indices_body_train, indices_body_test = train_test_split(X_body_tfidf, y, indices, test_size = 0.2, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4vqLmG5aa_U","colab_type":"code","colab":{}},"source":["df1.loc[indices_body_train].groupby('fake').agg('count')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZnZ9o3LabPV","colab_type":"code","colab":{}},"source":["df1.loc[indices_body_test].groupby('fake').agg('count')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQpiztVEabeG","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB\n","nb_body = MultinomialNB()\n","nb_body.fit(X_body_tfidf_train, y_body_train)\n","y_body_train_pred = nb_body.predict(X_body_tfidf_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUNn88n-ab3R","colab_type":"code","colab":{}},"source":["print('Naive Bayes In Training data F1 and Accuracy Scores:')\n","print('F1 score {:.4}%'.format(f1_score(y_body_train, y_body_train_pred, average='macro')*100 ))\n","print ('Accuracy score {:.4}%'.format(accuracy_score(y_body_train, y_body_train_pred)*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoBPD1Udbpfe","colab_type":"code","colab":{}},"source":["np.where(y_body_train != y_body_train_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JycJi2Dgbp2k","colab_type":"code","colab":{}},"source":["y_body_pred = nb_body.predict(X_body_tfidf_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SS1bZiLrbqOX","colab_type":"code","colab":{}},"source":["# print metrics\n","print('Naive Bayes Test F1 and Accuracy Scores:')\n","print('F1 score {:.4}%'.format(f1_score(y_body_test, y_body_pred, average='macro')*100 ))\n","print ('Accuracy score {:.4}%'.format(accuracy_score(y_body_test, y_body_pred)*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7K7okLabqed","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB\n","nb_body = MultinomialNB()\n","nb_body.fit(X_body_tfidf_train, y_body_train)\n","y_body_train_pred = nb_body.predict(X_body_tfidf_train)\n","\n","\n","\n","# model.fit(train.data, train.target)\n","# labels = model.predict(test.data)\n","\n","from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(y_body_train, y_body_train_pred)\n","sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","            xticklabels=X_body_tfidf_train.shape, yticklabels=y_body_train.shape)\n","plt.xlabel('true label')\n","plt.ylabel('predicted label');\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7z-6c-8Wbquu","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('punkt')\n","fake = df1[df1['fake']==1]\n","\n","spam_words = nltk.word_tokenize(\" \".join(fake['clean_text'].values.tolist()))\n","spam_counter = Counter(spam_words)\n","print(spam_counter.most_common(50))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdVSNc-tbq8B","colab_type":"code","colab":{}},"source":["spam_wordcloud = WordCloud(width=1200, height=1000, random_state = 42).generate(\" \".join(spam_words))\n","# wordcloud2 = WordCloud(width=1200, height=1000, collocations = False).generate(\" \".join(spam_words)) # to turn off bigrams\n","\n","fig = plt.figure(figsize=(20,10), facecolor = 'k')\n","plt.imshow(spam_wordcloud)\n","plt.axis('off')\n","plt.tight_layout(pad=0)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6NZua5FbrIx","colab_type":"code","colab":{}},"source":["spam_bigrams = nltk.bigrams(spam_words)\n","spam_counter = Counter(spam_bigrams)\n","print(spam_counter.most_common(10))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uS0IjGYSbrXD","colab_type":"code","colab":{}},"source":["ham = df1[df1['fake']==0]\n","ham_words = nltk.word_tokenize(\" \".join(ham['clean_text'].values.tolist()))\n","ham_counter = Counter(ham_words)\n","print(ham_counter.most_common(50))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8UgUspHbrom","colab_type":"code","colab":{}},"source":["ham_wordcloud = WordCloud(width=1200, height=1000, random_state = 42).generate(\" \".join(ham_words))\n","\n","fig = plt.figure(figsize=(20,10), facecolor = 'k' )\n","plt.imshow(ham_wordcloud)\n","plt.axis('off')\n","plt.tight_layout(pad=0)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqVRfIj7br3_","colab_type":"code","colab":{}},"source":["ham_bigrams = nltk.bigrams(ham_words)\n","ham_counter = Counter(ham_bigrams)\n","print(ham_counter.most_common(10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YlgrY1qGdFRk","colab_type":"text"},"source":["RNN"]},{"cell_type":"code","metadata":{"id":"d3L50oUteTrr","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from sklearn.metrics import confusion_matrix, plot_confusion_matrix, roc_curve, auc\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"57j1BhQZdHgK","colab_type":"code","colab":{}},"source":["df_original = pd.read_csv('news.csv')\n","\n","len_before = len(df_original['title'])\n","\n","#drop null values\n","df_original = df_original.dropna()\n","# dropping ALL duplicte values \n","df_original.drop_duplicates(subset =\"title\", keep = 'first', inplace = True) \n","\n","len_after = len(df_original['title'])\n","\n","print(f'number of rows removed: {len_before - len_after}')\n","\n","#df_original.head()\n","df_original.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZdQHzr00dHqj","colab_type":"code","colab":{}},"source":["#Combine the title and the text\n","combined_text = []\n","temp = ''\n","for elm in df_original['title']:\n","  combined_text.append(elm + ' <TITLETOTEXT> ')\n","count = 0\n","for elm in df_original['text']:\n","  combined_text[count] = (combined_text[count] + elm)\n","  count = count + 1\n","count = 0\n","\n","df_original['combined_text'] = combined_text\n","print(df_original['title'][0])\n","print(df_original['text'][0])\n","print(df_original['combined_text'][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGHNX4KedH1i","colab_type":"code","colab":{}},"source":["def get_equal_random_list(start, end, ratio):\n","  assert end > start, \"end parameter must be more than start parameter\"\n","  assert ratio >= 0.0 and ratio <= 1.0, \"ratio must be between 0 and 1\"\n","  random.seed(123)\n","  len_of_params = end - start\n","  num_of_elem = int(len_of_params * ratio)\n","  step = len_of_params / num_of_elem\n","  ret_list = []\n","  for i in range(0, num_of_elem):\n","    increment = int(step * i)\n","    ret_list.append(int(increment + (step * random.random())))\n","  random.shuffle(ret_list)\n","  return ret_list\n","'''\n","example input ---> get_equal_random_list(0, 100, 0.05)\n","output ---> [78, 98, 54, 5, 24]\n","formula ---> [0-19, 20-39, 40-59, 60-79, 80-99] in random order\n","\n","'''\n","print(get_equal_random_list(0, 100, 0.05))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytaglV4ndH5N","colab_type":"code","colab":{}},"source":["# Let us take only 10% of df to create our data set namely 'sample'\n","sample_size = 0.1\n","sample_combined_text = []\n","sample_fake = []\n","sample_list = get_equal_random_list(0, len(df_original['combined_text']), 0.05)\n","for i in sample_list:\n","  try:\n","    sample_combined_text.append(df_original['combined_text'][i])\n","    sample_fake.append(df_original['fake'][i])\n","  except:\n","    continue\n","sample = pd.DataFrame()\n","sample['combined_text'] = sample_combined_text\n","sample['fake'] = sample_fake\n","sample.head(10)\n","len(sample)\n","#sample = df.sample(int(0.1*len(df)))\n","#print(len(sample))\n","#sample.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOEbIXD4dH-7","colab_type":"code","colab":{}},"source":["# Let us create a tokenizer\n","tokenizer = tfds.features.text.Tokenizer()\n","vocabulary_set = set()\n","for text_tensor in df_original.combined_text.values:\n","  some_tokens = tokenizer.tokenize(text_tensor)\n","  vocabulary_set.update(some_tokens)\n","\n","vocab_size = len(vocabulary_set)\n","# How many unique words do we have?\n","print(vocab_size)\n","print(vocabulary_set)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpStWyiedIHj","colab_type":"code","colab":{}},"source":["test_size = 0.1\n","test_data = sample.head(int(len(sample) * test_size))\n","test_data.head()\n","print(test_data.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xteqt_2djvI","colab_type":"code","colab":{}},"source":["print(f'test: [0 - {int(len(sample) * test_size)}], train: [{int(len(sample) - (len(sample) * (1 - test_size)))} - {len(sample) - 1}]')\n","count = 0\n","for test in test_data['combined_text']:\n","  for train in train_data['combined_text']:\n","    if(train == test):\n","      print(f'equals at {count}\\n{train}\\n{test}')\n","  count = count + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiU4VnBRdj3r","colab_type":"code","colab":{}},"source":["print(f'test(true: {test_data.fake.values.tolist().count(0)}, fake: {test_data.fake.values.tolist().count(1)})')\n","print(f'train(true: {train_data.fake.values.tolist().count(0)}, fake: {train_data.fake.values.tolist().count(1)})')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MeZV9ryLdj_H","colab_type":"code","colab":{}},"source":["#datagen_sample = tf.data.Dataset.from_tensor_slices((sample.combined_text, sample.fake))\n","datagen = tf.data.Dataset.from_tensor_slices((df_original.combined_text, df_original.fake))\n","datagen_train = tf.data.Dataset.from_tensor_slices((train_data.combined_text, train_data.fake))\n","datagen_test = tf.data.Dataset.from_tensor_slices((test_data.combined_text, test_data.fake))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKtWSFTqdkH2","colab_type":"code","colab":{}},"source":["# Now we will create our encoder\n","# This encoder will encode text to numbers\n","encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n","\n","# Lets see encoder in action\n","samplecombined_text = sample.combined_text.values[0]\n","encodedcombined_text = encoder.encode(sample.combined_text.values[0])\n","decodedcombined_text = encoder.decode(encodedcombined_text)\n","print(f'sample combined_text : \\n{samplecombined_text}')\n","print(f'encode combined_text : \\n{encodedcombined_text}')\n","print(f'decode combined_text : \\n{decodedcombined_text}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKklIIfCdkQ8","colab_type":"code","colab":{}},"source":["def encode(text_tensor, label):\n","  encoded_text = encoder.encode(text_tensor.numpy())\n","  return encoded_text, label\n","\n","def encode_map_fn(text, label):\n","  # py_func doesn't set the shape of the returned tensors.\n","  encoded_text, label = tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n","\n","  # `tf.data.Datasets` works best if all components have a shape set\n","  #  so set the shapes manually: \n","  encoded_text.set_shape([None])\n","  label.set_shape([])\n","\n","  return encoded_text, label\n","\n","\n","\n","# Now let us map datagenerator to encode_map_fn\n","#sample_map = datagen.map(encode_map_fn)\n","\n","train_data = datagen_train.map(encode_map_fn)\n","train_data = train_data.padded_batch(50)\n","\n","test_data = datagen_test.map(encode_map_fn)\n","test_data = test_data.padded_batch(50)\n","\n","# Now we will break our sample_map to train and test (90% and 10%)\n","#test train split\n","#make sure there is no overlap\n","'''\n","train_data = sample_map.skip(int(len(sample) * 0.9))\n","train_data = train_data.padded_batch(50)\n","\n","test_data = sample_map.take(int(len(sample) * 0.1))\n","test_data = test_data.padded_batch(50)\n","print(train_data)\n","training_size = int(len(train_data))\n","testing_size = int(len(test_data))\n","print(f'training_size\\t: {training_size}\\ntesting_size\\t: {testing_size}')\n","'''\n","\n","data_map = datagen.map(encode_map_fn)\n","df_test = data_map.padded_batch(50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0iyvLPWdt82","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(1)\n","])\n","model.summary()\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])\n","\n","history = model.fit(train_data, epochs=15, validation_data=test_data)\n","test_loss, test_acc = model.evaluate(test_data)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtJpN4YgduJV","colab_type":"code","colab":{}},"source":["plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.title('Accuracy vs Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","\n","# plot train and validation loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model train vs validation loss')\n","plt.ylabel('loss')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'validation'], loc='upper right')\n","plt.show()\n","\n","test_loss, test_acc = model.evaluate(df_test)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ee2JoXGUduQ2","colab_type":"code","colab":{}},"source":["labels = ['True', 'False']\n","model_pred = (model.predict(df_test) > 0.5).astype(\"int32\")\n","#model_pred_sigmoid = model.predict(df_test)\n","\n","model_true = df_original.fake.values\n","print(model_pred)\n","print(model_true)\n","cmatrix = confusion_matrix(model_true, model_pred) #use labels name as well\n","\n","print(cmatrix)\n","sns.heatmap(data = cmatrix, xticklabels = labels, yticklabels = labels, annot = True, cbar = True, fmt = '0', cmap=\"YlGnBu\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYx2EKcUduUW","colab_type":"code","colab":{}},"source":["fpr, tpr, threshold = roc_curve(model_true, model_pred)\n","area = auc(fpr, tpr)\n","\n","import matplotlib.pyplot as plt\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b')\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()\n","print(f'Area Under Curve: {area}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSBJu4zDdubG","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1)\n","])\n","model.summary()\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])\n","\n","history = model.fit(train_data, epochs=10, validation_data=test_data)\n","\n","test_loss, test_acc = model.evaluate(test_data)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"02_g-Uerd-2I","colab_type":"code","colab":{}},"source":["plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.title('Accuracy vs Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","\n","# plot train and validation loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model train vs validation loss')\n","plt.ylabel('loss')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'validation'], loc='upper right')\n","plt.show()\n","\n","test_loss, test_acc = model.evaluate(df_test)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSoQHuxad--a","colab_type":"code","colab":{}},"source":["labels = ['True', 'False']\n","model_pred = (model.predict(df_test) > 0.5).astype(\"int32\")\n","#model_pred_sigmoid = model.predict(df_test)\n","\n","model_true = df_original.fake.values\n","print(model_pred)\n","print(model_true)\n","cmatrix = confusion_matrix(model_true, model_pred) #use labels name as well\n","\n","print(cmatrix)\n","sns.heatmap(data = cmatrix, xticklabels = labels, yticklabels = labels, annot = True, cbar = True, fmt = '0', cmap=\"YlGnBu\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSi_zbFpd_Hg","colab_type":"code","colab":{}},"source":["fpr, tpr, threshold = roc_curve(model_true, model_pred)\n","area = auc(fpr, tpr)\n","\n","import matplotlib.pyplot as plt\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b')\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()\n","print(f'Area Under Curve: {area}')"],"execution_count":0,"outputs":[]}]}